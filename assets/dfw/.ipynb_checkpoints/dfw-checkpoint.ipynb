{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import string\n",
      "import re\n",
      "import collections"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first, run pdftotext on this:\n",
      "#http://nkelber.com/engl295/wp-content/uploads/2012/07/David-Foster-Wallace-Infinite-Jest-v2.0.pdf\n",
      "raw = open(\"David-Foster-Wallace-Infinite-Jest-v2.0.txt\",'rU').read()\n",
      "\n",
      "rec = re.compile('[^A-Za-z ]')\n",
      "no_punct_lower = re.sub(rec,' ',raw).lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vocabulary_size(raw):\n",
      "    \"\"\"\n",
      "    Compute the number of distinct words (stemmed)\n",
      "    ignores non-ascii letters\n",
      "    \"\"\"\n",
      "    print(\"chars in raw: \",len(raw))\n",
      "    #remove non-ascii letters\n",
      "    rec = re.compile('[^A-Za-z ]')\n",
      "    no_punct_lower = re.sub(rec,' ',raw).lower()\n",
      "    \n",
      "    #tokenize, stem, and count\n",
      "    tokens=nltk.word_tokenize(no_punct_lower)[:35000]\n",
      "    print(\"tokens in raw: \",len(tokens))\n",
      "\n",
      "    stemmer = nltk.stem.PorterStemmer()\n",
      "    stemmed_tokens = []\n",
      "    for token in tokens:\n",
      "        stemmed_tokens.append(stemmer.stem(token))\n",
      "    return len(set(stemmed_tokens))\n",
      "\n",
      "print(\"words in vocabulary:\\t\",vocabulary_size(raw))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "chars in raw:  3204159\n",
        "tokens in raw: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 577608\n",
        "words in vocabulary:\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20584\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conjunctions = set(open('conjunctions.txt','rU').read().splitlines())\n",
      "print(\"num conjunctions in Wiktionary:\",len(conjunctions))\n",
      "\n",
      "prepositions = set(open('prepositions.txt','rU').read().splitlines())\n",
      "print(\"num prepositions in Wiktionary:\",len(prepositions))\n",
      "\n",
      "def all_uninterrupted_seqs(raw, prep_conj, min_seq_length=3):\n",
      "    \"\"\"\n",
      "    Given a string of text and a set of terms to search for\n",
      "    return all uninterrupted chains of the terms\n",
      "    \"\"\"\n",
      "    tokens = nltk.wordpunct_tokenize(raw)\n",
      "\n",
      "    all_seqs = []\n",
      "    for idx,token in enumerate(tokens):\n",
      "        tokl = token.lower()\n",
      "        if tokl in prep_conj:\n",
      "            seq = [tokl]\n",
      "            n_ahead = 0\n",
      "            while tokl in prep_conj:\n",
      "                n_ahead += 1\n",
      "                tokl = tokens[idx+n_ahead].lower()\n",
      "                if tokl in prep_conj:\n",
      "                    seq.append(tokl)\n",
      "            if len(seq) >= min_seq_length:\n",
      "                all_seqs.append(seq)\n",
      "    return all_seqs\n",
      "\n",
      "\n",
      "def longest_seq(seqs):\n",
      "    \"\"\"\n",
      "    Find the longest chain in the output of all_uninterrupted_seqs\n",
      "    \"\"\"\n",
      "    max_len = 0\n",
      "    max_seq = []\n",
      "    for seq in seqs:\n",
      "        if len(seq) >= max_len:\n",
      "            max_seq = seq\n",
      "            max_len = len(max_seq)\n",
      "    return max_seq\n",
      "\n",
      "#the type of terms to search for chains of\n",
      "term_set = prepositions.union(conjunctions)\n",
      "\n",
      "minlen=3\n",
      "chains = all_uninterrupted_seqs(raw, term_set, minlen)\n",
      "chainsp1 = all_uninterrupted_seqs(raw, term_set, minlen+1)\n",
      "chainsp2 = all_uninterrupted_seqs(raw, term_set, minlen+2)\n",
      "\n",
      "n=10\n",
      "top_conjs = collections.Counter(list(map(lambda x: ' '.join(x),chains))).most_common(n)\n",
      "top_conjsp1 = collections.Counter(list(map(lambda x: ' '.join(x),chainsp1))).most_common(n)\n",
      "top_conjsp2 = collections.Counter(list(map(lambda x: ' '.join(x),chainsp2))).most_common(n)\n",
      "\n",
      "#print out the longest chain\n",
      "top_seq = longest_seq(chains)\n",
      "print(\"\\nlongest sequence:\\t\",len(top_seq), \" \".join(top_seq), \"\\n\")\n",
      "\n",
      "for i in range(n):\n",
      "    try:\n",
      "        print(\"|\",top_conjs[i][0],':|',top_conjs[i][1], '|',top_conjsp1[i][0],':|',top_conjsp1[i][1], '|',\n",
      "          top_conjsp2[i][0],':|',top_conjsp2[i][1], '|')\n",
      "    except IndexError:\n",
      "        print(\"|\",top_conjs[i][0],':|',top_conjs[i][1], '|',top_conjsp1[i][0],':|',top_conjsp1[i][1], '|',\" \"\n",
      "          ,'|',\" \", '|')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num conjunctions in Wiktionary: 191\n",
        "num prepositions in Wiktionary: 391\n",
        "\n",
        "longest sequence:\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13 again / and again and again / and again and again and again \n",
        "\n",
        "| and out of :| 41 | in and out of :| 30 | and so on and so :| 9 |\n",
        "| and so on :| 39 | so on and so :| 11 | on out and over to :| 1 |\n",
        "| up and down :| 32 | and so on and so :| 9 | up and out and down and :| 1 |\n",
        "| in and out of :| 30 | over and over again :| 6 | and up and down through :| 1 |\n",
        "| over and over :| 28 | up and down in :| 4 | and down over and over for :| 1 |\n",
        "| up next to :| 24 | to come in and :| 4 | but so but then so :| 1 |\n",
        "| and / or :| 22 | come and gone and :| 3 | as in like not to :| 1 |\n",
        "| on and on :| 15 | over and over and :| 3 | only that at times like :| 1 |\n",
        "| up out of :| 14 | over and over for :| 3 | not to come down or :| 1 |\n",
        "| as in like :| 12 | up and down both :| 3 | about wanting to and so on :| 1 |\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def acronyms(raw):\n",
      "    rec = re.compile('[^A-Z\\.]')\n",
      "    caps = re.sub(rec,' ',raw)\n",
      "    toks = nltk.tokenize.word_tokenize(caps)\n",
      "    acks = filter(lambda x:('.' in x) and re.search('[A-Z]',x) and len(x) > 8, toks)\n",
      "    return collections.Counter(acks)\n",
      "\n",
      "acks = acronyms(raw)\n",
      "\n",
      "top_acks = acks.most_common(20)\n",
      "for i in range(20):\n",
      "    print('|',top_acks[i][0],\":|\",top_acks[i][1],\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "| O.N.A.N.T.A :| 31 \n",
        "| P.G.O.A.T :| 13 \n",
        "| U.S.O.U.S :| 11 \n",
        "| E.M.P.H.H :| 11 \n",
        "| U.S.S.M.K :| 7 \n",
        "| O.N.A.N.C.A.A :| 5 \n",
        "| U.S.B.S.S :| 3 \n",
        "| Y.T.S.D.B :| 2 \n",
        "| Y.D.P.A.H :| 2 \n",
        "| ESCHAX.DIR :| 1 \n",
        "| A.A.O.A.A :| 1 \n",
        "| O.N.A.N.F.L :| 1 \n",
        "| Y.W.Q.M.D :| 1 \n",
        "| O.N.A.N.M.A :| 1 \n",
        "| F.O.P.P.P :| 1 \n",
        "| N.A.A.U.P :| 1 \n",
        "| O.N.A.N.D.E.A :| 1 \n",
        "| O.N.A.N.. :| 1 \n",
        "| B.A.M.E.S :| 1 \n",
        "| I.B.P.W.D.W :| 1 \n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_ack = ''\n",
      "for ack in set(acks):\n",
      "    if len(ack) > len(max_ack):\n",
      "        max_ack = ack\n",
      "        print(max_ack)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A.F.B\n",
        "M.I.A.D\n",
        "A.A.O.A.A\n",
        "O.N.A.N.T.A\n",
        "O.N.A.N.D.E.A\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brwn = nltk.corpus.brown.raw()[:3204159]\n",
      "print(\"words in brown corpus:\\t\",vocabulary_size(brwn))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "chars in raw:  3204159\n",
        "tokens in raw: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 667190\n",
        "words in brown corpus:\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15771\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "# finder = nltk.collocations.BigramCollocationFinder.from_words(raw)\n",
      "# list(filter(lambda x: max(len(x[0]),len(x[1]))>2,finder.nbest(bigram_measures.pmi, 300000)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}