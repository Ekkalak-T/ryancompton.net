{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import string\n",
      "import re\n",
      "import collections"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first, run pdftotext on this:\n",
      "#http://nkelber.com/engl295/wp-content/uploads/2012/07/David-Foster-Wallace-Infinite-Jest-v2.0.pdf\n",
      "raw = open(\"David-Foster-Wallace-Infinite-Jest-v2.0.txt\",'rU').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vocabulary_size(raw):\n",
      "    \"\"\"\n",
      "    Compute the number of distinct words (stemmed)\n",
      "    ignores non-ascii letters\n",
      "    \"\"\"\n",
      "    #remove non-ascii letters\n",
      "    rec = re.compile('[^A-Za-z ]')\n",
      "    no_punct_lower = re.sub(rec,' ',raw).lower()\n",
      "    \n",
      "    #tokenize, stem, and count\n",
      "    tokens=nltk.word_tokenize(no_punct_lower)\n",
      "    stemmer = nltk.stem.PorterStemmer()\n",
      "    stemmed_tokens = []\n",
      "    for token in tokens:\n",
      "        stemmed_tokens.append(stemmer.stem(token))\n",
      "    return len(set(stemmed_tokens))\n",
      "\n",
      "print(\"words in vocabulary:\\t\",vocabulary_size(raw))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "words in vocabulary:\t 20584\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#http://www.englishclub.com/vocabulary/common-conjunctions-25.htm\n",
      "conjunctions = set(open('conjunctions.txt','rU').read().splitlines())\n",
      "print(\"num conjunctions in Wiktionary:\",len(conjunctions))\n",
      "\n",
      "prepositions = set(open('prepositions.txt','rU').read().splitlines())\n",
      "print(\"num prepositions in Wiktionary:\",len(prepositions))\n",
      "\n",
      "def longest_uninterrupted_seqs(raw, terms, min_seq_length=3):\n",
      "    \"\"\"\n",
      "    Given a string of text and a set of terms to search for\n",
      "    return a count of how often uninterrupted seqs appear\n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(raw)\n",
      "    longest_seq = []\n",
      "    all_seqs = []\n",
      "    for idx,token in enumerate(tokens):\n",
      "        tokl = token.lower()\n",
      "        if tokl in conjunctions:\n",
      "            seq = [(idx, tokl)]\n",
      "            n_ahead = 0\n",
      "            while tokl in conjunctions:\n",
      "                n_ahead += 1\n",
      "                tokl = tokens[idx+n_ahead].lower()\n",
      "                if tokl in conjunctions:\n",
      "                    seq.append((idx+n_ahead, tokl))\n",
      "            if len(seq) > len(longest_seq):\n",
      "                longest_seq = seq\n",
      "            if len(seq) > min_seq_length:\n",
      "                all_seqs.append(str(list(map(lambda x: x[1], seq))))\n",
      "    \n",
      "    #print most common conjunction trains\n",
      "    for common in collections.Counter(all_seqs).most_common(10):\n",
      "        print(common[0],common[1])\n",
      "        \n",
      "    return longest_seq\n",
      "\n",
      "longest_seq = longest_uninterrupted_seqs(raw, conjunctions, 5)\n",
      "print(\"longest sequence of common conjunctions\", longest_seq)\n",
      "\n",
      "conj_start = min(list(map(lambda x: x[0], longest_seq)))\n",
      "print(\" \".join(tokens[(conj_start-20):(conj_start+40)]) )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnboundLocalError",
       "evalue": "local variable 'n_ahead' referenced before assignment",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-118-c9ad94cc8b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlongest_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mlongest_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlongest_uninterrupted_seqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconjunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"longest sequence of common conjunctions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongest_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-118-c9ad94cc8b01>\u001b[0m in \u001b[0;36mlongest_uninterrupted_seqs\u001b[0;34m(raw, terms, min_seq_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtokl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconjunctions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mn_ahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtokl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconjunctions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'n_ahead' referenced before assignment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num conjunctions in Wiktionary: 192\n",
        "num prepositions in Wiktionary: 392\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.collocations.demo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}