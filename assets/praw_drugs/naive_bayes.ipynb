{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn\n",
      "import nltk\n",
      "import sqlalchemy\n",
      "import sqlite3\n",
      "import collections\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sqlite3 connection\n",
      "dbname = '/home/aahu/Dropbox/ryancompton.net/assets/praw_drugs/drugs.db'\n",
      "conn = sqlalchemy.create_engine('sqlite+pysqlite:///'+dbname, module=sqlite3.dbapi2)\n",
      "\n",
      "def load_subreddit(tablename, conn):\n",
      "    df = pd.read_sql(tablename, conn)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize.stanford import StanfordTokenizer\n",
      "def my_tokenize(text):\n",
      "    return nltk.wordpunct_tokenize(text)\n",
      "    #return StanfordTokenizer().tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load the data\n",
      "subs = ['lsd','Benzodiazepines','opiates','cripplingalcoholism','cocaine']\n",
      "dfs = []\n",
      "for sub in subs:\n",
      "    df = load_subreddit(sub, conn)\n",
      "    df = df.head(3000)\n",
      "    dfs.append(df)\n",
      "df = pd.concat(dfs)\n",
      "\n",
      "#map drugs to colors\n",
      "cs = ['b','g','r','c','m','y','k']\n",
      "cdict = {}\n",
      "for idx, sub in enumerate(subs):\n",
      "    cdict[sub] = cs[idx%len(cs)]\n",
      "df['color'] = df['subreddit'].map(lambda x:cdict[x])\n",
      "\n",
      "\n",
      "def build_tfidf_transformer(docs = [], tokenizer=my_tokenize, max_doc_count=2000, vocab_limit=10000):\n",
      "    \"\"\"\n",
      "    fit tfidf vocabulary\n",
      "    max_doc_count tosses out words that are too common (e.g. 'dude')\n",
      "    \"\"\"\n",
      "    if max_doc_count is not None:\n",
      "        max_doc_f = float(max_doc_count)/len(docs)\n",
      "        print(max_doc_f)\n",
      "    else:\n",
      "        max_doc_f = 1.0\n",
      "    tfidf = sklearn.feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer, \n",
      "                                                        stop_words='english',\n",
      "                                                        max_df=max_doc_f,\n",
      "                                                        max_features=vocab_limit)\n",
      "    tfidf.fit_transform(docs)\n",
      "    \n",
      "    return tfidf\n",
      "\n",
      "tfidf = build_tfidf_transformer(df['body'], tokenizer=my_tokenize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.15553308966482618\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy\n",
      "def sparse_to_dict(mat, fnames):\n",
      "    \"\"\"\n",
      "    convert scipy.sparse (which is what sklean uses)\n",
      "    to dict (which is easier for nltk)\n",
      "    usually fnames is from tfidf.get_feature_names()\n",
      "    \"\"\"\n",
      "    cx = scipy.sparse.coo_matrix(mat)\n",
      "    d = {}\n",
      "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
      "        d[fnames[j]] = v\n",
      "    return d\n",
      "\n",
      "def features_from_messages(messages, label, feature_extractor, **kwargs):\n",
      "    '''\n",
      "    Make a (features, label) tuple for each message in a list of a certain,\n",
      "    label of e-mails ('spam', 'ham') and return a list of these tuples.\n",
      "\n",
      "    Note every e-mail in 'messages' should have the same label.\n",
      "    '''\n",
      "    features_labels = []\n",
      "    feature_vecs = feature_extractor(messages)\n",
      "    for feature_vec in feature_vecs:\n",
      "        features_labels.append((feature_vec, label))\n",
      "    return features_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opiates_txt = df[df['subreddit']=='opiates']['body']\n",
      "opiate_vecs = features_from_messages(opiates_txt, label='opitates',feature_extractor=tfidf.transform)\n",
      "\n",
      "non_opiates_txt = df[df['subreddit'] != 'opiates']['body']\n",
      "sample_rows = np.random.choice(non_opiates_txt.index.values, len(opiates_txt), replace=False) #downsample to balance\n",
      "non_opiates_txt = non_opiates_txt.iloc[sample_rows]\n",
      "non_opiate_vecs = features_from_messages(non_opiates_txt, label='non_opitates',feature_extractor=tfidf.transform)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# #dim reduce plots...\n",
      "# from tsne import bh_sne\n",
      "\n",
      "# print(len(opiate_vecs))\n",
      "# print(len(non_opiate_vecs))\n",
      "\n",
      "# big_arr = []\n",
      "# colrs = []\n",
      "# for opiate_vec in opiate_vecs[:500]:\n",
      "#     big_arr.append(opiate_vec[0].todense())\n",
      "#     colrs.append('r')\n",
      "# for non_opiate_vec in non_opiate_vecs[:500]:\n",
      "#     big_arr.append(non_opiate_vec[0].todense())\n",
      "#     colrs.append('k')\n",
      "\n",
      "# big_arr = np.vstack(big_arr)    \n",
      "\n",
      "# X_2d = bh_sne(big_arr.transpose())\n",
      "# xys = X_2d\n",
      "# plt.figure()\n",
      "# plt.scatter(xys[:,0], xys[:,1], c=colrs, linewidths=0, alpha=.7)\n",
      "# plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3000\n",
        "3000\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_train_split(feature_vecs, hold_out_frac=0.1):\n",
      "    test_idxs = np.random.choice(range(len(feature_vecs)), int(len(feature_vecs)*hold_out_frac), replace=False)\n",
      "    test_idxs = list(test_idxs)\n",
      "    train_idxs = [idx for idx in range(len(feature_vecs)) if idx not in test_idxs]\n",
      "    \n",
      "    test_out = [feature_vecs[idx] for idx in test_idxs] \n",
      "    train_out = [feature_vecs[idx] for idx in train_idxs] \n",
      "    \n",
      "    return test_out, train_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_vecs_to_nltk_fmt(feature_vecs, transformer):\n",
      "    out = []\n",
      "    for fv, lbl in feature_vecs:\n",
      "        out.append((sparse_to_dict(fv,transformer),lbl))\n",
      "    return out\n",
      "\n",
      "def check_classifier(test_pos, train_pos, test_neg, train_neg):\n",
      "    '''\n",
      "    Train the classifier on the training spam and ham, then check its accuracy\n",
      "    on the test data, and show the classifier's most informative features.\n",
      "    '''\n",
      "    train_set = train_pos+train_neg\n",
      "    # Train the classifier on the training set\n",
      "    classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "    \n",
      "    # How accurate is the classifier on the test sets?\n",
      "    print('Test positive labels accuracy: {0:.2f}%'\n",
      "       .format(100 * nltk.classify.accuracy(classifier, test_pos)))\n",
      "    print('Test negative labels accuracy: {0:.2f}%'\n",
      "       .format(100 * nltk.classify.accuracy(classifier, test_neg)))\n",
      "\n",
      "    # Show the top 20 informative features\n",
      "    print(classifier.show_most_informative_features(20))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_opiates, train_opiates = test_train_split(opiate_vecs)\n",
      "test_non_opiates, train_non_opiates = test_train_split(non_opiate_vecs)\n",
      "\n",
      "test_opiates = feature_vecs_to_nltk_fmt(test_opiates,tfidf.get_feature_names())\n",
      "train_opiates = feature_vecs_to_nltk_fmt(train_opiates,tfidf.get_feature_names())\n",
      "test_non_opiates = feature_vecs_to_nltk_fmt(test_non_opiates,tfidf.get_feature_names())\n",
      "train_non_opiates = feature_vecs_to_nltk_fmt(train_non_opiates,tfidf.get_feature_names())\n",
      "\n",
      "check_classifier(test_opiates,train_opiates, test_non_opiates,train_non_opiates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test positive labels accuracy: 99.00%\n",
        "Test negative labels accuracy: 52.33%\n",
        "Most Informative Features\n",
        "                    took = 1.0            non_op : opitat =      4.3 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                      :) = 1.0            non_op : opitat =      3.0 : 1.0\n",
        "                     yep = 1.0            non_op : opitat =      2.3 : 1.0\n",
        "                  saying = 1.0            non_op : opitat =      2.3 : 1.0\n",
        "                       ! = 1.0            non_op : opitat =      1.7 : 1.0\n",
        "                 careful = 1.0            non_op : opitat =      1.7 : 1.0\n",
        "                       r = 0.60374282863763706 non_op : opitat =      1.7 : 1.0\n",
        "                    good = 0.53595358838515639 non_op : opitat =      1.7 : 1.0\n",
        "                    luck = 0.84424744660382278 non_op : opitat =      1.7 : 1.0\n",
        "                       / = 0.79717914979546767 non_op : opitat =      1.7 : 1.0\n",
        "                     yes = 1.0            non_op : opitat =      1.3 : 1.0\n",
        "                    trip = None           opitat : non_op =      1.2 : 1.0\n",
        "                     lsd = None           opitat : non_op =      1.2 : 1.0\n",
        "                tripping = None           opitat : non_op =      1.1 : 1.0\n",
        "                    acid = None           opitat : non_op =      1.1 : 1.0\n",
        "                       - = None           opitat : non_op =      1.1 : 1.0\n",
        "              experience = None           opitat : non_op =      1.1 : 1.0\n",
        "                    time = None           opitat : non_op =      1.0 : 1.0\n",
        "                       ( = None           opitat : non_op =      1.0 : 1.0\n",
        "                    mind = None           opitat : non_op =      1.0 : 1.0\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_from_message"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "{'!!!!': 0.41201491818926911,\n",
        " 'nipple': 0.52446664893275696,\n",
        " 'looks': 0.32373635523399658,\n",
        " 'picture': 0.36993091970632708,\n",
        " 'nice': 0.28083938934892066,\n",
        " 'leg': 0.4844105342391905}"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}