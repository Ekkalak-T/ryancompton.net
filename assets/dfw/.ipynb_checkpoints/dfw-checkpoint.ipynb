{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import string\n",
      "import re\n",
      "import collections"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first, run pdftotext on this:\n",
      "#http://nkelber.com/engl295/wp-content/uploads/2012/07/David-Foster-Wallace-Infinite-Jest-v2.0.pdf\n",
      "raw = open(\"David-Foster-Wallace-Infinite-Jest-v2.0.txt\",'rU').read()\n",
      "\n",
      "rec = re.compile('[^A-Za-z ]')\n",
      "no_punct_lower = re.sub(rec,' ',raw).lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vocabulary_size(raw):\n",
      "    \"\"\"\n",
      "    Compute the number of distinct words (stemmed)\n",
      "    ignores non-ascii letters\n",
      "    \"\"\"\n",
      "    #remove non-ascii letters\n",
      "    rec = re.compile('[^A-Za-z ]')\n",
      "    no_punct_lower = re.sub(rec,' ',raw).lower()\n",
      "    \n",
      "    #tokenize, stem, and count\n",
      "    tokens=nltk.word_tokenize(no_punct_lower)\n",
      "    stemmer = nltk.stem.PorterStemmer()\n",
      "    stemmed_tokens = []\n",
      "    for token in tokens:\n",
      "        stemmed_tokens.append(stemmer.stem(token))\n",
      "    return len(set(stemmed_tokens))\n",
      "\n",
      "print(\"words in vocabulary:\\t\",vocabulary_size(raw))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "words in vocabulary:\t 20584\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conjunctions = set(open('conjunctions.txt','rU').read().splitlines())\n",
      "print(\"num conjunctions in Wiktionary:\",len(conjunctions))\n",
      "\n",
      "prepositions = set(open('prepositions.txt','rU').read().splitlines())\n",
      "print(\"num prepositions in Wiktionary:\",len(prepositions))\n",
      "\n",
      "def all_uninterrupted_seqs(raw, prep_conj, min_seq_length=3):\n",
      "    \"\"\"\n",
      "    Given a string of text and a set of terms to search for\n",
      "    return a count of how often uninterrupted seqs appear\n",
      "    \"\"\"\n",
      "    tokens = nltk.wordpunct_tokenize(raw)\n",
      "\n",
      "    all_seqs = []\n",
      "    for idx,token in enumerate(tokens):\n",
      "        tokl = token.lower()\n",
      "        if tokl in prep_conj:\n",
      "            seq = [tokl]\n",
      "            n_ahead = 0\n",
      "            while tokl in prep_conj:\n",
      "                n_ahead += 1\n",
      "                tokl = tokens[idx+n_ahead].lower()\n",
      "                if tokl in prep_conj:\n",
      "                    seq.append(tokl)\n",
      "            if len(seq) >= min_seq_length:\n",
      "                all_seqs.append(seq)\n",
      "    return all_seqs\n",
      "\n",
      "\n",
      "def longest_seq(seqs):\n",
      "    \"\"\"\n",
      "    Find the longest seq in the output of all_uninterrupted_seqs\n",
      "    \"\"\"\n",
      "    max_len = 0\n",
      "    max_seq = []\n",
      "    for seq in seqs:\n",
      "        if len(seq) >= max_len:\n",
      "            max_seq = seq\n",
      "            max_len = len(max_seq)\n",
      "    return max_seq\n",
      "\n",
      "#the type of terms to search for chains of\n",
      "term_set = prepositions\n",
      "\n",
      "minlen=3\n",
      "chains = all_uninterrupted_seqs(raw, term_set, minlen)\n",
      "chainsp1 = all_uninterrupted_seqs(raw, term_set, minlen+1)\n",
      "chainsp2 = all_uninterrupted_seqs(raw, term_set, minlen+2)\n",
      "\n",
      "n=10\n",
      "top_conjs = collections.Counter(list(map(lambda x: ' '.join(x),chains))).most_common(n)\n",
      "top_conjsp1 = collections.Counter(list(map(lambda x: ' '.join(x),chainsp1))).most_common(n)\n",
      "top_conjsp2 = collections.Counter(list(map(lambda x: ' '.join(x),chainsp2))).most_common(n)\n",
      "\n",
      "#print out the longest chain\n",
      "top_seq = longest_seq(chains)\n",
      "print(\"\\nlongest sequence:\\t\",len(top_seq), \" \".join(top_seq), \"\\n\")\n",
      "\n",
      "for i in range(n):\n",
      "    try:\n",
      "        print(\"|\",top_conjs[i][0],':|',top_conjs[i][1], '|',top_conjsp1[i][0],':|',top_conjsp1[i][1], '|',\n",
      "          top_conjsp2[i][0],':|',top_conjsp2[i][1], '|')\n",
      "    except IndexError:\n",
      "        print(\"|\",top_conjs[i][0],':|',top_conjs[i][1], '|',top_conjsp1[i][0],':|',top_conjsp1[i][1], '|',\" \"\n",
      "          ,'|',\" \", '|')\n",
      "        \n",
      "          \n",
      "          #, top_preps[i][0], \"\\t\", top_preps[i][1], '|',\n",
      "          #top_either[i][0], '\\t', top_either[i][1], \"|\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num conjunctions in Wiktionary: 191\n",
        "num prepositions in Wiktionary: 391\n",
        "\n",
        "longest sequence:\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 but and so and but so \n",
        "\n",
        "| up next to :| 24 | to come out of :| 2 | in over from behind like :| 1 |\n",
        "| up out of :| 14 | to come in to :| 2 |   |   |\n",
        "| as in like :| 13 | to come up to :| 1 |   |   |\n",
        "| come out of :| 9 | around with down in :| 1 |   |   |\n",
        "| to come to :| 8 | to come up with :| 1 |   |   |\n",
        "| come up with :| 7 | over from behind like :| 1 |   |   |\n",
        "| out from under :| 6 | come out of on :| 1 |   |   |\n",
        "| come in to :| 6 | in from out over :| 1 |   |   |\n",
        "| come in for :| 5 | as in with like :| 1 |   |   |\n",
        "| to come in :| 5 | in over from behind like :| 1 |   |   |\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def acronyms(raw):\n",
      "    rec = re.compile('[^A-Z\\.]')\n",
      "    caps = re.sub(rec,' ',raw)\n",
      "    toks = nltk.tokenize.word_tokenize(caps)\n",
      "    acks = filter(lambda x:('.' in x) and re.search('[A-Z]',x) and len(x) > 8, toks)\n",
      "    return collections.Counter(acks)\n",
      "\n",
      "acks = acronyms(raw)\n",
      "print('\\n'.join(map(lambda x:str(x),acks.most_common(20))))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('O.N.A.N.T.A', 31)\n",
        "('P.G.O.A.T', 13)\n",
        "('E.M.P.H.H', 11)\n",
        "('U.S.O.U.S', 11)\n",
        "('U.S.S.M.K', 7)\n",
        "('O.N.A.N.C.A.A', 5)\n",
        "('U.S.B.S.S', 3)\n",
        "('Y.D.P.A.H', 2)\n",
        "('Y.T.S.D.B', 2)\n",
        "('Y.W.Q.M.D', 1)\n",
        "('O.N.A.N.F.L', 1)\n",
        "('F.O.P.P.P', 1)\n",
        "('N.A.A.U.P', 1)\n",
        "('O.N.A.N..', 1)\n",
        "('O.N.A.N.M.A', 1)\n",
        "('B.A.M.E.S', 1)\n",
        "('ESCHAX.DIR', 1)\n",
        "('I.B.P.W.D.W', 1)\n",
        "('O.N.A.N.D.E.A', 1)\n",
        "('A.A.O.A.A', 1)\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_ack = ''\n",
      "for ack in set(acks):\n",
      "    if len(ack) > len(max_ack):\n",
      "        max_ack = ack\n",
      "        print(max_ack)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A.F.B\n",
        "M.I.A.D\n",
        "A.A.O.A.A\n",
        "O.N.A.N.T.A\n",
        "O.N.A.N.D.E.A\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l = \"kk\"\n",
      "len(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "finder = nltk.collocations.BigramCollocationFinder.from_words(raw)\n",
      "list(filter(lambda x: max(len(x[0]),len(x[1]))>2,finder.nbest(bigram_measures.pmi, 300000)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}