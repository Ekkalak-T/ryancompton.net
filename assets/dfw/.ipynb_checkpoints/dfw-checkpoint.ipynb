{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.stem import *\n",
      "import string\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first, run pdftotext on this:\n",
      "#http://nkelber.com/engl295/wp-content/uploads/2012/07/David-Foster-Wallace-Infinite-Jest-v2.0.pdf\n",
      "raw = open(\"David-Foster-Wallace-Infinite-Jest-v2.0.txt\",'rU').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vocabulary_size(raw):\n",
      "    \"\"\"\n",
      "    Compute the number of distinct words (stemmed)\n",
      "    ignores punctuation/capitalization\n",
      "    \"\"\"\n",
      "    #remove punctuation and set everything to lower case\n",
      "    #exclude = set(string.punctuation).union(set(string.digits))\n",
      "    #rec = re.compile('[A-z]')\n",
      "    no_punct = ''.join(ch for ch in raw if ch in string.ascii_letters )\n",
      "    no_punct_lower = no_punct.lower()\n",
      "\n",
      "    #tokenize, stem, and count\n",
      "    tokens=nltk.word_tokenize(no_punct_lower)  \n",
      "    stemmer = PorterStemmer()\n",
      "    stemmed_tokens = []\n",
      "    for token in tokens:\n",
      "        stemmed_tokens.append(stemmer.stem(token))\n",
      "  \n",
      "    return len(set(stemmed_tokens))\n",
      "\n",
      "print(\"words in vocabulary:\\t\",vocabulary_size(raw))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-47-9b225adff6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words in vocabulary:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-47-9b225adff6d4>\u001b[0m in \u001b[0;36mvocabulary_size\u001b[0;34m(raw)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#tokenize, stem, and count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_punct_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     91\u001b[0m     along with :class:`.PunktSentenceTokenizer`).\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     return [token for sent in sent_tokenize(text)\n\u001b[0m\u001b[1;32m     94\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m     81\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \"\"\"\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \"\"\"\n\u001b[0;32m-> 1318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \"\"\"\n\u001b[1;32m   1347\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \"\"\"\n\u001b[1;32m    353\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/nltk-3.0a4-py3.4.egg/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#http://www.englishclub.com/vocabulary/common-conjunctions-25.htm\n",
      "conjunctions = set(open('conjunctions.txt','rU').read().splitlines())\n",
      "print(\"num conjunctions in Wiktionary:\",len(conjunctions))\n",
      "\n",
      "\n",
      "def longest_uninterrupted_seqs(raw, terms, min_seq_length=3):\n",
      "    \"\"\"\n",
      "    Given a string of text and a set of terms to search for\n",
      "    return a count of how often uninterrupted seqs appear\n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(raw)\n",
      "    longest_seq = []\n",
      "    for idx,token in enumerate(tokens):\n",
      "        tokl = token.lower()\n",
      "        if tokl in conjunctions:\n",
      "            seq = []\n",
      "            n_ahead = 0\n",
      "            while tokl in conjunctions:\n",
      "                tokl = tokens[idx+n_ahead].lower()\n",
      "                seq.append((idx+n_ahead, tokl))\n",
      "                n_ahead += 1\n",
      "            if len(seq) > len(longest_seq):\n",
      "                longest_seq = seq\n",
      "    return longest_seq\n",
      "\n",
      "longest_seq = longest_uninterrupted_seqs(raw, conjunctions)\n",
      "print(\"longest sequence of common conjunctions\", longest_seq)\n",
      "\n",
      "conj_start = min(list(map(lambda x: x[0], longest_seq)))\n",
      "print(\"in book: \", tokens[(conj_start-20):(conj_start+40)])\n",
      "#print tokens[400:405]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num conjunctions in Wiktionary: 192\n",
        "longest sequence of common conjunctions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [(538885, 'but'), (538886, 'and'), (538887, 'so'), (538888, 'and'), (538889, 'but'), (538890, 'so'), (538891, \"i'm\")]\n",
        "in book:  ['a', 'sister', 'up', 'so', 'bad', 'I', 'ca', \"n't\", 'hardly', 'see', 'to', 'get', 'the', 'truck', 'off', 'the', 'lawn', 'and', 'leave', '.', 'But', 'and', 'so', 'and', 'but', 'so', \"I'm\", 'driving', 'back', 'home', ',', 'and', 'I', \"'m\", 'so', 'mad', 'I', 'all', 'of', 'a', 'sudden', 'try', 'and', 'pray', '.', 'And', 'I', 'try', 'and', 'pray', ',', 'driving', 'along', 'and', 'whatnot', ',', 'and', 'it', 'comes', 'to']\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.collocations.demo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "firefox.txt\n",
        "\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['download manager', 'http ://', 'new tab', 'context menu', 'address bar', 'print preview', 'location bar', 'mozilla firebird', 'bookmarks toolbar', 'password manager', 'new window', 'status bar', 'full screen', 'dom inspector', 'personal toolbar']\n",
        "\t Correlation to raw_freq: 0.6021"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "grail.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t ['black knight', 'clop clop', 'head knight', 'mumble mumble', 'squeak squeak', 'saw saw', 'holy grail', 'run away', 'french guard', 'cartoon character', 'iesu domine', 'pie iesu', 'round table', 'sir robin', 'clap clap']\n",
        "\t Correlation to raw_freq: 0.7946\n",
        "overheard.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['teen girl', 'new york', 'teen boy', 'little boy', 'little girl', 'last night', 'old lady', 'old man', 'bus driver', 'long island', 'high school', 'drunk guy', 'look like', 'hipster girl', 'flight attendant']\n",
        "\t Correlation to raw_freq: 0.5210"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "pirates.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t ['jack sparrow', 'elizabeth swann', 'davy jones', 'flying dutchman', 'lord cutler', 'cutler beckett', 'black pearl', 'tia dalma', 'cannibal island', 'port royal', 'bamboo pole', 'edinburgh trader', 'east india', 'india trading', 'wounded sailor']\n",
        "\t Correlation to raw_freq: 0.6932\n",
        "singles.txt\n",
        "\t ['would like', 'age open', 'medium build', 'social drinker', 'non smoker', 'quiet nights', 'long term', 'easy going', 'poss rship', 'financially secure', 'fun times', 'weekends away', 'seeks lady', 'single dad', 'similar interests']\n",
        "\t Correlation to raw_freq: 0.6389\n",
        "wine.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\t ['top ***', 'bare ****', 'bare ***', 'good length', 'bare ***(*)', 'medium weight', 'needs time', 'pretty good', 'mature claret', 'nice balance', 'red fruits', 'rather good', 'white burgundy', 'quite forward', 'top ****']\n",
        "\t Correlation to raw_freq: 0.5347"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NLTK Downloader\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Download which package (l=list; x=cancel)?\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    Downloading package stopwords to /home/ryan/nltk_data...\n",
        "      Unzipping corpora/stopwords.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Packages:\n",
        "  [ ] abc................. Australian Broadcasting Commission 2006\n",
        "  [ ] alpino.............. Alpino Dutch Treebank\n",
        "  [ ] basque_grammars..... Grammars for Basque\n",
        "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
        "                           Extraction Systems in Biology)\n",
        "  [ ] book_grammars....... Grammars from NLTK Book\n",
        "  [ ] brown............... Brown Corpus\n",
        "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
        "  [ ] cess_cat............ CESS-CAT Treebank\n",
        "  [ ] cess_esp............ CESS-ESP Treebank\n",
        "  [ ] chat80.............. Chat-80 Data Files\n",
        "  [ ] city_database....... City Database\n",
        "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
        "  [ ] comtrans............ ComTrans Corpus Sample\n",
        "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
        "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
        "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
        "                           and Basque Subset)\n",
        "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
        "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
        "                           Corpus\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] floresta............ Portuguese Treebank\n",
        "  [ ] framenet_v15........ FrameNet 1.5\n",
        "  [ ] gazetteers.......... Gazeteer Lists\n",
        "  [ ] genesis............. Genesis Corpus\n",
        "  [ ] gutenberg........... Project Gutenberg Selections\n",
        "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
        "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
        "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
        "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
        "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
        "                           ChaSen format)\n",
        "  [ ] kimmo............... PC-KIMMO Data Files\n",
        "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
        "  [ ] langid.............. Language Id Corpus\n",
        "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
        "                           for parser comparison\n",
        "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
        "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
        "                           part-of-speech tags\n",
        "  [ ] machado............. Machado de Assis -- Obra Completa\n",
        "  [ ] masc_tagged......... MASC Tagged Corpus\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
        "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
        "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
        "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
        "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
        "  [ ] nps_chat............ NPS Chat\n",
        "  [ ] oanc_masc........... Open American National Corpus: Manually\n",
        "                           Annotated Sub-Corpus\n",
        "  [ ] omw................. Open Multilingual Wordnet\n",
        "  [ ] paradigms........... Paradigm Corpus\n",
        "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
        "                           Evaluation Shared Task\n",
        "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
        "  [ ] pl196x.............. Polish language of the XX century sixties\n",
        "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
        "  [ ] problem_reports..... Problem Report Corpus\n",
        "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
        "  [ ] ptb................. Penn Treebank\n",
        "  [*] punkt............... Punkt Tokenizer Models\n",
        "  [ ] qc.................. Experimental Data for Question Classification\n",
        "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
        "                           version\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
        "                           Portuguesa)\n",
        "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
        "  [ ] sample_grammars..... Sample Grammars\n",
        "  [ ] semcor.............. SemCor 3.0\n",
        "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
        "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
        "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
        "  [ ] smultron............ SMULTRON Corpus Sample\n",
        "  [ ] spanish_grammars.... Grammars for Spanish\n",
        "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
        "  [*] stopwords........... Stopwords Corpus\n",
        "  [ ] swadesh............. Swadesh Wordlists\n",
        "  [ ] switchboard......... Switchboard Corpus Sample\n",
        "  [ ] tagsets............. Help on Tagsets\n",
        "  [ ] timit............... TIMIT Corpus Sample\n",
        "  [ ] toolbox............. Toolbox Sample Files\n",
        "  [ ] treebank............ Penn Treebank Sample\n",
        "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
        "                           (Unicode Version)\n",
        "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] unicode_samples..... Unicode Samples\n",
        "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
        "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
        "  [ ] webtext............. Web Text Corpus\n",
        "  [ ] wordnet............. WordNet\n",
        "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
        "  [ ] words............... Word Lists\n",
        "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
        "                           English Prose\n",
        "\n",
        "Collections:\n",
        "  [P] all-corpora......... All the corpora\n",
        "  [P] all................. All packages\n",
        "  [P] book................ Everything used in the NLTK Book\n",
        "\n",
        "([*] marks installed packages; [P] marks partially installed collections)\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}